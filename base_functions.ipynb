{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_urls=['https://www1.nseindia.com/homepage/Indices1.json','https://www1.nseindia.com/live_market/dynaContent/live_analysis/gainers/niftyGainers1.json'\n",
    "          ,'https://archives.nseindia.com/content/equities/series_change.csv'\n",
    "   \n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import math\n",
    "\n",
    "from urllib.parse import quote\n",
    "\n",
    "import traceback\n",
    "import time, sched\n",
    "from datetime import timedelta,datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from tabulate import tabulate \n",
    "from IPython.display import display,HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import json_normalize  \n",
    "\n",
    "import pdfkit\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
    "import tempfile\n",
    "import base64\n",
    "from html import unescape,escape\n",
    "\n",
    "from matplotlib.ticker import PercentFormatter \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm,colors\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import cufflinks as cf\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "nse_base_url='https://www.nseindia.com/'\n",
    "nse1_base_url='https://www1.nseindia.com/'\n",
    "\n",
    "ticker_url='https://www.nseindia.com/api/quote-equity'\n",
    "ticker_param = {'symbol':'TANLA'}\n",
    "\n",
    "chrome_headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Whale/0.10.36.11 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "    # 'Host': 'www.matchesfashion.com',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Accept-Encoding':'gzip, deflate'}\n",
    "\n",
    "\n",
    "my_initial_port = {}\n",
    "source_symb = 'SYMBOL'\n",
    "target_symb = 'i_symbol'\n",
    "closing_price_keys= ['pr_close','pr_lastPrice']\n",
    "\n",
    "default_interested_columns = [target_symb,'pr_pChange','pr_lastPrice','pr_open','pr_previousClose'\n",
    "                      ,'pr_close','m_series','pr_pBand','i_companyName','m_industry','request_time']\n",
    "\n",
    "base_cookies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show up to 15 cols, 50 rows by default\n",
    "#pd.set_option('display.max_cols', 15)\n",
    "#pd.set_option('display.max_rows', 50)\n",
    "# Suitable default display for floats\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "np.set_printoptions(formatter={'float_kind':\"{:.3f}\".format})\n",
    "cf.go_offline()\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6, 6)\n",
    "# This one is optional -- change graphs to SVG\n",
    "# Only use if you don't have a lot of points/lines\n",
    "# on your graph.\n",
    "# Can also just use ['retina'] if you don't want SVG.\n",
    "%config InlineBackend.figure_formats = ['retina', 'svg']\n",
    "\n",
    "default_layout = cf.Layout(\n",
    "    height=600,\n",
    "    width=600\n",
    ")\n",
    "\n",
    "#Adding wkhtmltox to path for pdf gen\n",
    "os.environ[\"PATH\"] = os.environ[\"PATH\"] + os.pathsep + 'path\\\\wkhtmltox\\\\bin'\n",
    "\n",
    "#\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation Function , base metrics\n",
    "\n",
    "\n",
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    percentile_.__name__ = str(n)+'%'\n",
    "    return percentile_\n",
    "\n",
    "def last(x):\n",
    "    return x.tail(1)\n",
    "\n",
    "def first(x):\n",
    "    return x.head(1)\n",
    "\n",
    "def p_change(x):\n",
    "    return (x.tail(1).values[0]-x.head(1).values[0])*100/x.head(1).values[0]\n",
    "\n",
    "concat = lambda x: ' '.join(x.unique())\n",
    "\n",
    "time_dims = { \n",
    "    '1 week' : lambda x: x['time'] > datetime.today() - timedelta(days=7),\n",
    "    '1 month' : lambda x: x['time'] > datetime.today() - timedelta(days=30),\n",
    "    #'3 month' : lambda x: x['time'] > datetime.today() - timedelta(days=90),\n",
    "    '6 month' :  lambda x: x['time'] > datetime.today() - timedelta(days=180),\n",
    "    '1 year' : lambda x: x['time'] > datetime.today() - timedelta(days=365),\n",
    "    '5 year' : lambda x: x['time'] > datetime.today() - timedelta(days=365*5),\n",
    "    #'Overall' : lambda x: True,\n",
    "            }\n",
    "\n",
    "metrics =[np.mean, np.std, 'min', 'max',first,last, percentile(25),percentile(50), percentile(75), percentile(95),p_change]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyURL:\n",
    "    def __init__(self, name, url,content_type='application/json',node=\"data\",data=None,parent_page=nse_base_url,payload={}):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.content_type=content_type\n",
    "        self.node=node\n",
    "        self.data = data\n",
    "        self.parent_page = parent_page\n",
    "        self.payload = payload\n",
    "    def set_data(data):\n",
    "        self.data=data\n",
    "    def info():\n",
    "        print(str(self))\n",
    "        display(data)\n",
    "    def __str__(self):\n",
    "        return \" \".join([self.name,self.url,str(self.payload)]);\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_map = {'priceInfo':'pr_','info':'i_','metadata':'m_','pr_pPriceBand':'pr_pBand_'}\n",
    "# Flatten json\n",
    "def flatten_json(nested_json, exclude=['']):\n",
    "    \"\"\"Flatten json object with nested keys into a single level.\n",
    "        Args:\n",
    "            nested_json: A nested json object.\n",
    "            exclude: Keys to exclude from output.\n",
    "        Returns:\n",
    "            The flattened json object if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name='', exclude=exclude):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                if a not in exclude: flatten(x[a], get_prefix(name + str(a) ))\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(nested_json)\n",
    "    return out\n",
    "\n",
    "def get_prefix(key):\n",
    "    return key_map.get(key,key+'_')\n",
    "\n",
    "def display_df_html(df):\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "def display_full(df):\n",
    "    with pd.option_context(\"display.max_rows\", df.shape[0],'display.max_columns', df.shape[1]):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP based function\n",
    "\n",
    "def loadURL(myurl,latest):\n",
    "    print('loading url ', myurl)\n",
    "    response=requests.get(myurl.url,params=myurl.payload,cookies=get_cookies(myurl.parent_page),headers=chrome_headers,timeout=20)\n",
    "    print(str(myurl),response)\n",
    "    s = response.text\n",
    "    try:\n",
    "        if(myurl.content_type=='application/json'):\n",
    "            myurl.data = json_normalize(pd.read_json(io.StringIO(s))[myurl.node])\n",
    "        elif(myurl.content_type=='json'):\n",
    "            myurl.data = response.json()\n",
    "        elif(myurl.content_type=='csv'):\n",
    "            myurl.data = pd.read_csv(io.StringIO(s))\n",
    "        elif(myurl.content_type=='html'):\n",
    "            myurl.data = pd.read_html(s)\n",
    "        else:\n",
    "            myurl.data=s\n",
    "        return myurl.data\n",
    "    except Exception:\n",
    "        print('Error parsing data ',s)\n",
    "\n",
    "def get_cookies(parent_url=nse_base_url):\n",
    "    global base_cookies\n",
    "    if(parent_url not in base_cookies):\n",
    "        print('loading cookies for',parent_url)\n",
    "        base_cookies[parent_url] = requests.get(parent_url,headers=chrome_headers)\n",
    "        display(base_cookies)\n",
    "\n",
    "    return base_cookies[parent_url].cookies\n",
    "    \n",
    "def load_ticker_info(ticker):\n",
    "    global base_cookies\n",
    "    print('loading ticker info',ticker_url,ticker)\n",
    "    payload = {'symbol':ticker}\n",
    "    ticker_data = None\n",
    "    try:\n",
    "        r = requests.get(ticker_url,params=payload,headers=chrome_headers,cookies=get_cookies())\n",
    "        ticker_data = flatten_json(r.json());\n",
    "        ticker_data['request_time'] = datetime.today()\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        base_cookies = {} \n",
    "    \n",
    "    return ticker_data\n",
    "\n",
    "def load_index_data(index_name):\n",
    "    url = 'https://www.nseindia.com/api/equity-stockIndices?csv=false&index='+quote(index_name)\n",
    "    data_url = MyURL(index_name,url,content_type='data',parent_page=nse_base_url)\n",
    "    text = loadURL(data_url,False)\n",
    "    #print(text)\n",
    "    text = data_url.data.replace(' \\n','',13)\n",
    "    nifty_next_50 = pd.read_csv(io.StringIO(text[text.find('\"'):]))  #[['SYMBOL']]\n",
    "    nifty_next_50 = nifty_next_50.iloc[1:]\n",
    "    return nifty_next_50\n",
    "\n",
    "def load_ca_info(days=365):\n",
    "    #base_url = 'https://api.moneycontrol.com/mcapi/v1/stock/corporate-action?deviceType=W&scId=TCS&section=d&start=0&limit=10'\n",
    "    #nse_dividend_url='https://www1.nseindia.com/corporates/datafiles/CA_TCS_LAST_12_MONTHS.csv'\n",
    "    nse_dividend_all ='https://www.nseindia.com/api/corporates-corporateActions'\n",
    "    to_time = datetime.today().strftime(\"%d-%m-%Y\")\n",
    "    from_time = (datetime.today() - timedelta(days=days)).strftime(\"%d-%m-%Y\")\n",
    "    payload = {'index':'equities','csv':'true','from_date':from_time,'to_date':to_time}\n",
    "    data_url = MyURL('Dividend',nse_dividend_all,payload=payload,content_type='data',parent_page=nse_base_url)\n",
    "    text = loadURL(data_url,False)\n",
    "    data_frame = pd.read_csv(io.StringIO(text[text.find('\"'):]))\n",
    "    data_frame.rename(columns={\"Symbol\": \"SYMBOL\"},inplace=True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe filter function\n",
    "def time_filter(target_df,time_key):\n",
    "    return target_df[target_df.apply(time_dims[time_key],axis=1)].reset_index()\n",
    "\n",
    "def ticker_filter(target_df,ticker):\n",
    "    return target_df[target_df['SYMBOL']==ticker].reset_index()\n",
    "\n",
    "def apply_filter(data,column,value):\n",
    "    return data[data[column]==value].reset_index()\n",
    "\n",
    "def order_time_dims(data,time_dim_column):\n",
    "    data[time_dim_column] = pd.Categorical(data[time_dim_column],ordered=True,categories=time_dims.keys())\n",
    "    return data\n",
    "def filter_by_index(df,column='index',values=[]):\n",
    "    return df[df.index.get_level_values(column).isin(values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "buy_tax_overhead = 0.0\n",
    "sell_tax_overhead = buy_tax_overhead\n",
    "\n",
    "def __count_days(x):\n",
    "    if (pd.isnull(x[1])):\n",
    "        return (datetime.today().date()-x[0]).days\n",
    "    return (x[1] -x[0]).days\n",
    "\n",
    "# File based portfolio load/update function\n",
    "def load_portfolio_file(file,populate_avg=True):\n",
    "    if(os.path.isfile('output/'+file)):\n",
    "        file_df = pd.read_csv ('output/'+file)\n",
    "    else:\n",
    "        file_df = pd.read_csv (file)\n",
    "\n",
    "    if (populate_avg and 'AVG' not in file_df.columns):\n",
    "        file_df['AVG'] = file_df['INVESTMENT']/file_df['QTY'] \n",
    "    \n",
    "    if(\"DATE\" in file_df.columns):\n",
    "        file_df['DATE'] =  pd.to_datetime(file_df['DATE']).dt.date\n",
    "        file_df['SELL_DATE'] =  pd.to_datetime(file_df['SELL_DATE']).dt.date\n",
    "        file_df['DAYS'] = file_df[['DATE','SELL_DATE']].apply(__count_days,axis=1)\n",
    "        file_df['CATEGORY'] = file_df['DAYS'].apply(trade_time_category)\n",
    "    \n",
    "    return file_df\n",
    "def get_today_dir():\n",
    "    today_dir = \"output/\"+datetime.today().strftime('%Y%m%d')+\"/\"\n",
    "    Path(today_dir).mkdir(parents=True, exist_ok=True)\n",
    "    return today_dir\n",
    "def update_portfolio_file(portfolio_df,file,update_current_port=True):\n",
    "    \n",
    "    portfolio_df.to_csv(get_today_dir()+file,index=False)\n",
    "    if(update_current_port):\n",
    "        portfolio_df[[\"SYMBOL\",\"DATE\",\"INVESTMENT\",\"REAL\",\"QTY\",\"SELL_DATE\"]].to_csv('output/'+file,index=False)\n",
    "\n",
    "\n",
    "def buy_position(file,symbol,qty,avg,date):\n",
    "    port_data = load_portfolio_file(file)\n",
    "    invst = (qty * avg) + buy_tax_overhead\n",
    "    port_data = port_data.append({'DATE':date,'SYMBOL':symbol,'REAL':0,'QTY':qty,'INVESTMENT':invst,'SELL_DATE':np.nan}, ignore_index=True)\n",
    "    update_portfolio_file(port_data,file,True)\n",
    "    return port_data\n",
    "   \n",
    "def __update_sell_position(ticker_record,computed_qty,avg,date):\n",
    "    if (ticker_record['QTY'] <=0) :\n",
    "        return ticker_record\n",
    "    \n",
    "    if (ticker_record['QTY'] - computed_qty > 0):\n",
    "        print('This need to be handled - split record')\n",
    "        investment_rem =  ticker_record['INVESTMENT'] - (ticker_record['AVG'] * computed_qty)\n",
    "        return ticker_record\n",
    "    \n",
    "    amt = computed_qty * avg - sell_tax_overhead\n",
    "    ticker_record['QTY'] =  ticker_record['QTY'] - computed_qty\n",
    "    ticker_record['REAL'] = ticker_record['REAL'] + amt - (ticker_record['AVG'] * computed_qty)\n",
    "    ticker_record['SELL_DATE'] = date\n",
    "    \n",
    "    \n",
    "    \n",
    "    return ticker_record\n",
    "\n",
    "def sell_position(portfolio_name,ticker,qty,avg,date):\n",
    "    port_data = load_portfolio_file(portfolio_name)\n",
    "     \n",
    "    ticker_records = ticker_filter(port_data, ticker).drop('index',axis=1).sort_values(['DATE'],ascending=True)\n",
    "    display(ticker_records)\n",
    "    updated_ticker_records = pd.DataFrame({})\n",
    "    remaining_qty = qty\n",
    "    for index, row in ticker_records.iterrows():\n",
    "        computed_qty = min(row['QTY'],remaining_qty)\n",
    "        remaining_qty = remaining_qty - computed_qty\n",
    "        updated_ticker_records = updated_ticker_records.append(__update_sell_position(row,computed_qty,avg,date))\n",
    "    \n",
    "    #ticker_record['DATE'] = date\n",
    "    port_data = port_data[port_data['SYMBOL']!= ticker].append(updated_ticker_records)\n",
    "        \n",
    "    update_portfolio_file(port_data,portfolio_name)\n",
    "    return port_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade_time_category(days):\n",
    "    if (days < 60):\n",
    "        return '< 2 Month'\n",
    "    elif (days < 366):\n",
    "        return '< 1 Year'\n",
    "    elif (days < 1830):\n",
    "        return '> 1 Year'\n",
    "    else:\n",
    "        return '> 5 Year'\n",
    "\n",
    "\n",
    "default_content_type='text/plain'\n",
    "\n",
    "def output_to_pdf(cell_results,file_name):\n",
    "    \n",
    "    final_pdf_string = ''\n",
    "    for cell_result in cell_results:\n",
    "        for output in cell_result.outputs:\n",
    "            final_pdf_string = final_pdf_string + get_pdf_output_string(output)\n",
    "        final_pdf_string = final_pdf_string + '<br><div style=\"page-break-before:always\">&nbsp;</div>'\n",
    "\n",
    "    print('writing to file now')        \n",
    "    pdfkit.from_string(final_pdf_string, get_today_dir() + file_name)\n",
    "    \n",
    "def get_pdf_output_string(output):\n",
    "    is_not_printed = True\n",
    "    for content_type in output.data:\n",
    "        if ('text/html' == content_type):\n",
    "            return output.data[content_type]\n",
    "            is_not_printed = False\n",
    "        elif ('image/svg+xml' == content_type):\n",
    "            return '<img src=\"data:image/svg+xml,'+ escape(output.data[content_type]) +'\"> </img>'\n",
    "            is_not_printed = False\n",
    "    \n",
    "    if(is_not_printed and (len(output.data)>1 or default_content_type not in output.data )):\n",
    "        print('None of the content_type handling, so using  for now ',output.data.keys())\n",
    "    \n",
    "    if(is_not_printed and default_content_type in output.data):        \n",
    "        return '<pre>'+output.data[default_content_type] +'</pre>'\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "\n",
    "breaker_text = '<br>'\n",
    "def add_page_break():\n",
    "    display(HTML(breaker_text))\n",
    "\n",
    "\n",
    "def create_pdf(content,file_name):\n",
    "    print ('new file creation',file_name)\n",
    "    pdfkit.from_string(content, file_name)\n",
    "    return file_name\n",
    "\n",
    "def pdf_concat(input_files, target_file):\n",
    "    input_streams = []\n",
    "    try:\n",
    "        # First open all the files, then produce the output file, and\n",
    "        # finally close the input files. This is necessary because\n",
    "        # the data isn't read from the input files until the write\n",
    "        # operation. Thanks to\n",
    "        # https://stackoverflow.com/questions/6773631/problem-with-closing-python-pypdf-writing-getting-a-valueerror-i-o-operation/6773733#6773733\n",
    "        display('files to concat',input_files)\n",
    "        with open(target_file,'wb') as file:\n",
    "            for input_file in input_files:\n",
    "                input_streams.append(open(input_file, 'rb'))\n",
    "            writer = PdfFileWriter()\n",
    "            for reader in map(PdfFileReader, input_streams):\n",
    "                for n in range(reader.getNumPages()):\n",
    "                    writer.addPage(reader.getPage(n))\n",
    "            writer.write(file)\n",
    "    finally:\n",
    "        for f in input_streams:\n",
    "            f.close()\n",
    "            \n",
    "def write_output_to_pdf(cell_results, final_file_name):    \n",
    "    tmp_dir = tempfile.TemporaryDirectory(dir = \"output/\")\n",
    "    try:\n",
    "        today_dir = \"output/\"+datetime.today().strftime('%Y%m%d')+\"/\"\n",
    "        default_content_type='text/plain'\n",
    "        final_pdf_string = ''\n",
    "        file_names = []\n",
    "        file_name = tmp_dir.name + '/tmp_pdf_'+str(len(file_names)) + '.pdf'\n",
    "        for cell_result in cell_results:\n",
    "            for output in cell_result.outputs:\n",
    "                current_string = get_pdf_output_string(output)\n",
    "                if(current_string == breaker_text):\n",
    "                    file_names.append(create_pdf(final_pdf_string, file_name))\n",
    "                    file_name = tmp_dir.name + '/tmp_pdf_'+str(len(file_names)) + '.pdf'\n",
    "                    final_pdf_string = ''\n",
    "                else:\n",
    "                    final_pdf_string = final_pdf_string + current_string\n",
    "\n",
    "        file_names.append(create_pdf(final_pdf_string, file_name))     \n",
    "        \n",
    "        pdf_concat(file_names,get_today_dir() + final_file_name )\n",
    "    finally:\n",
    "        tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nse_cached_data = {}\n",
    "\n",
    "def get_latest_data(tickers,latest):\n",
    "    global nse_cached_data\n",
    "    is_market_latest = True\n",
    "    \n",
    "    if(len(nse_cached_data)!=0):\n",
    "        load_timing = pd.DataFrame(list(nse_cached_data.values()))['request_time'].agg(['min','max'])\n",
    "    else:\n",
    "        load_timing = []\n",
    "    \n",
    "    # do not refresh data for sat and sun\n",
    "    if(datetime.today().weekday() > 4 or latest == False or (len(load_timing) > 0 and load_timing[0].hour >= 16)):\n",
    "        is_market_latest = False\n",
    "\n",
    "    if(datetime.today().hour >= 18 and len(load_timing) > 0 and load_timing[0].hour < 18):\n",
    "        is_market_latest = True\n",
    "        \n",
    "    available_cached_data = {}\n",
    "    for symb in tickers:\n",
    "        if (is_market_latest or symb not in nse_cached_data):\n",
    "            ticker_data = load_ticker_info(symb)\n",
    "            if(ticker_data is not None):\n",
    "                nse_cached_data[symb] = ticker_data\n",
    "            \n",
    "        if(symb in nse_cached_data):\n",
    "            available_cached_data[symb]= nse_cached_data[symb]\n",
    "    \n",
    "    return available_cached_data.values()\n",
    "    \n",
    "def populate_nse_data(my_port_df,latest=True,interested_columns=default_interested_columns):\n",
    "    nse_current_data = get_latest_data(my_port_df['SYMBOL'].unique(),latest)\n",
    "    new_df = pd.DataFrame(nse_current_data,columns=interested_columns)\n",
    "    new_df = new_df.rename(columns={target_symb: source_symb})\n",
    "    my_port_value = my_port_df.merge(new_df)\n",
    "    return my_port_value\n",
    "\n",
    "def enrich_portfolio(my_port_value):\n",
    "    start_time = datetime.today()\n",
    "    price_key=closing_price_keys[1]\n",
    "    if (my_port_value[closing_price_keys[0]].unique()[0]!=0):\n",
    "        price_key = closing_price_keys[0]\n",
    "    display(HTML('<b>Share Price Key: </b>'+price_key))\n",
    "    today_val_formule = 'QTY * '+ price_key\n",
    "    my_port_value.insert(3, 'MKT_VALUE', my_port_value.eval(today_val_formule))\n",
    "    my_port_value.insert(4, 'CHANGE', my_port_value.eval('MKT_VALUE - QTY * pr_previousClose'))\n",
    "    my_port_value.insert(5, 'UNREAL', my_port_value.eval('MKT_VALUE-INVESTMENT'))\n",
    "    my_port_value.insert(6, 'UNREAL %', my_port_value.eval('(MKT_VALUE - INVESTMENT) *100 /INVESTMENT'))\n",
    "    my_port_value['request_time'] = start_time\n",
    "    my_port_value['request_time'] = pd.to_datetime(my_port_value['request_time'])\n",
    "    return my_port_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_data(portfolio_df,daily_time_line):\n",
    "    market_data = load_price_history(portfolio_df['SYMBOL'].unique(),days=1)\n",
    "    port_data = pd.DataFrame({})\n",
    "    for item in market_data:\n",
    "        port_data = port_data.append(item.data,ignore_index=True)\n",
    "    \n",
    "    port_data = port_data.reset_index()[['SYMBOL','c','request_time']]\n",
    "    \n",
    "    if('pr_lastPrice' in portfolio_df):\n",
    "        portfolio_df = portfolio_df.drop(['pr_lastPrice'],axis=1)\n",
    "    if('request_time' in portfolio_df):\n",
    "        portfolio_df = portfolio_df.drop(['request_time'],axis=1) \n",
    "        \n",
    "    \n",
    "    portfolio_df = portfolio_df.merge(port_data)\n",
    "    portfolio_df = portfolio_df.rename(columns={'c':'pr_lastPrice'})\n",
    "    portfolio_df['pr_pChange'] = portfolio_df.eval('(pr_lastPrice - pr_previousClose)*100/pr_previousClose ')\n",
    "    portfolio_df = enrich_portfolio(portfolio_df)\n",
    "    #portfolio_df = portfolio_df.merge(raw_port)\n",
    "    # update portfolio\n",
    "    if('load_time' not in daily_time_line or daily_time_line['load_time'].max() != portfolio_df['load_time'].max()):\n",
    "        update_portfolio_file(portfolio_df,portfolio_name,False)\n",
    "        daily_time_line = daily_time_line.append(portfolio_df[portfolio_df['QTY']>0])\n",
    "        max_count = len(portfolio_df) * 60\n",
    "        if(len(daily_time_line) > max_count):\n",
    "            daily_time_line = daily_time_line.groupby(['SYMBOL','DATE']).head(10).append(daily_time_line.groupby(['SYMBOL','DATE']).tail(40))\n",
    "    return portfolio_df,daily_time_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_interval = '30' #in minute \n",
    "def load_price_history(tickers,days=3650,time_interval=\"1D\"):\n",
    "    market_price_url ='https://priceapi.moneycontrol.com/techCharts/techChartController/history'\n",
    "    market_base_url='https://www.moneycontrol.com/'\n",
    "    start_time = datetime.today()\n",
    "    market_data = []\n",
    "    to_time = int(datetime.today().timestamp())\n",
    "    from_time = int((datetime.today() - timedelta(days=days)).timestamp())\n",
    "    # Create URL data\n",
    "    for symbol in tickers:\n",
    "        payload = {'symbol':symbol,'resolution':time_interval,'from': from_time,'to':to_time}\n",
    "        market_data.append(MyURL(symbol,market_price_url,payload=payload,content_type='json',parent_page=market_base_url))\n",
    "    # load market data\n",
    "    for url in market_data:\n",
    "        \n",
    "        json = loadURL(url,True)\n",
    "        try:\n",
    "            del json['s']\n",
    "            #display(json['h'][1:10])\n",
    "            ticker_df = pd.DataFrame()\n",
    "            for element in json:\n",
    "                ticker_df[element] = json[element]\n",
    "\n",
    "            ticker_df['time'] = ticker_df['t'].apply(lambda t: datetime.fromtimestamp(t)) \n",
    "            ticker_df['SYMBOL'] = url.name\n",
    "            ticker_df['request_time'] = start_time\n",
    "            ticker_df['request_time'] = pd.to_datetime(ticker_df['request_time'])\n",
    "            url.data = ticker_df\n",
    "        except:\n",
    "            print('Error loading url ',url)\n",
    "            \n",
    "    print(len(market_data))        \n",
    "    return market_data\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_agg_col = {\n",
    "            'INVESTMENT':\"sum\",\n",
    "            'REAL': \"sum\",\n",
    "            'MKT_VALUE':\"sum\",\n",
    "            'CHANGE':\"sum\",\n",
    "            'UNREAL': \"sum\",\n",
    "        }\n",
    "\n",
    "indus_agg_col = port_agg_col.copy()\n",
    "indus_agg_col['SYMBOL'] = concat\n",
    "\n",
    "overall_agg_col = port_agg_col.copy()\n",
    "for key in overall_agg_col:\n",
    "    overall_agg_col[key] = [overall_agg_col[key]]\n",
    "    \n",
    "def _color_red_or_green(val):\n",
    "    color = 'red' if val < 0 else 'green'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "def display_intraday_change(daily_time_line):\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for ticker in daily_time_line['SYMBOL'].unique():\n",
    "        ticker_data = ticker_filter(daily_time_line,ticker)\n",
    "        ticker_data = ticker_data.groupby(['request_time','SYMBOL']).agg(port_agg_col).reset_index()\n",
    "        fig.add_trace(go.Scatter(x=ticker_data['request_time'], y=ticker_data['CHANGE'],\n",
    "                            #mode='lines+markers',\n",
    "                            mode='lines',\n",
    "                            name=ticker))\n",
    "    agg_data = daily_time_line.groupby('request_time').agg({'CHANGE':'sum','UNREAL':'sum'})\n",
    "    start_bar = 2\n",
    "    end_bar = 5\n",
    "    if (len(agg_data) > (start_bar+end_bar)):\n",
    "        agg_data = agg_data.head(start_bar).append(agg_data.tail(end_bar)).reset_index()\n",
    "    else:\n",
    "        agg_data = agg_data.reset_index()\n",
    "    #agg_data['width'] = 1.5\n",
    "    fig.add_trace(go.Bar(x=agg_data['request_time'],y=agg_data['CHANGE'],marker_color='indianred',name='--Today--'))\n",
    "    \n",
    "    #fig.update_xaxes(\n",
    "    #    rangebreaks=[dict(values=daily_time_line['request_time'].unique())] # hide dates with no values\n",
    "    #)\n",
    "    #fig.add_trace(go.Bar(x=agg_data['request_time'],y=agg_data['UNREAL'],marker_color='darkblue',name='--OverAll--'))\n",
    "    \n",
    "    #fig.update_layout(\n",
    "    #yaxis = dict(\n",
    "    #        tickmode = 'array',\n",
    "    #        tickvals = [-600,-300,-50,0,100, 300, 500, 1000, 3000, 5000, 10000]\n",
    "    #    )\n",
    "    #)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "def profit_info(sold_port_df):\n",
    "    overall_agg = sold_port_df.agg({\n",
    "            'INVESTMENT':[\"sum\"],\n",
    "            'REAL': [\"sum\"]})\n",
    "    year_agg = sold_port_df.groupby('CATEGORY').agg({\n",
    "            'INVESTMENT':\"sum\",\n",
    "            'REAL': \"sum\"})\n",
    "    display(year_agg)\n",
    "    display(overall_agg)\n",
    "    \n",
    "def basic_port_info(portfolio_df):\n",
    "    load_timing = portfolio_df['request_time'].agg(['min','max'])\n",
    "    display(HTML('<b>Timing:</b> <I>'+ load_timing[0].strftime('%d %b %Y [ %H:%M:%S - ') + load_timing[1].strftime('%H:%M:%S ]') +'</I>'))\n",
    "    \n",
    "    overall_agg = portfolio_df.agg(overall_agg_col)\n",
    "    year_agg = portfolio_df.groupby('CATEGORY').agg(port_agg_col)\n",
    "    symbol_agg = portfolio_df.groupby('SYMBOL').agg(port_agg_col)\n",
    "    \n",
    "    industry_agg = portfolio_df[portfolio_df['QTY']>0].groupby('m_industry').agg(indus_agg_col)\n",
    "    \n",
    "    #viridis / inferno / plasma / magma\n",
    "    cmap = 'viridis'\n",
    "    #fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10,5))\n",
    "    #plt.subplots_adjust(bottom=.1)\n",
    "    \n",
    "    display_full(portfolio_df.set_index(['CATEGORY','SYMBOL']).sort_values(['CATEGORY','UNREAL %'],ascending=[False,True]))\n",
    "    \n",
    "    layout = {'height':400,'width':800,'hovermode': 'x'}\n",
    "    industry_agg[[\"INVESTMENT\",\"MKT_VALUE\",\"REAL\"]].iplot(kind='bar',layout=layout,subplots=False)\n",
    "    \n",
    "    symbol_agg[[\"INVESTMENT\",\"MKT_VALUE\",\"REAL\"]].iplot(kind='bar',subplots=False,layout=layout)\n",
    "    \n",
    "    #for ax in axes:\n",
    "        #plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    #plt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    #plt.xticks(rotation=30)\n",
    "    #fig.delaxes(axes[1][0])\n",
    "    #fig.delaxes(axes[1][1])\n",
    "    #fig.tight_layout()\n",
    "    #plt.show()\n",
    "    \n",
    "             #.style.applymap(_color_red_or_green, subset=['UNREAL','CHANGE'])\n",
    "    display(industry_agg)\n",
    "    display(year_agg)\n",
    "    display(overall_agg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
